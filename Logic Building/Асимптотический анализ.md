
Если у нас есть два алгоритма для одной задачи, как понять, какой из них лучше?

Наивный способ — реализовать оба алгоритма и запускать их на компьютере с разными входными данными, сравнивая время выполнения. У этого подхода есть много проблем при анализе алгоритмов:

- Возможно, что для некоторых входных данных первый алгоритм работает лучше второго, а для других — наоборот.
    
- Возможно, что на одном компьютере первый алгоритм работает быстрее, а на другом — второй.
    

**Асимптотический анализ** решает эти проблемы. При асимптотическом анализе мы оцениваем эффективность алгоритма **в зависимости от размера входных данных**, не измеряя фактическое время выполнения. Мы определяем **порядок роста времени (или памяти)** алгоритма относительно размера входа.

Например:

- Линейный поиск растет **линейно** с размером входа
    
- Бинарный поиск растет **логарифмически** с размером входа
    

---

### Пример

Рассмотрим задачу поиска элемента в **отсортированном массиве**.

Возможные решения:

- **Линейный поиск** (порядок роста — линейный)
    
- **Бинарный поиск** (порядок роста — логарифмический)
    

Почему асимптотический анализ помогает?

Предположим:

- Линейный поиск выполняется на компьютере A
    
- Бинарный поиск выполняется на компьютере B
    

Для небольших размеров массива `n` компьютер A может справляться быстрее. Но после определенного значения `n` бинарный поиск **обязательно** станет быстрее линейного, даже если он работает на более медленном компьютере.

Почему так происходит?

- Порядок роста бинарного поиска относительно входа — **логарифмический**, а линейного поиска — **линейный**.
    
- Машинные константы (скорость компьютера) становятся несущественными при больших значениях `n`.
    

Например, пусть:

- константа для машины A = 0.2
    
- константа для машины B = 1000
    

Т.е. A в 5000 раз мощнее B.

|Размер входа|Время выполнения на A|Время выполнения на B|
|---|---|---|
|10|2 с|~1 ч|
|100|20 с|~1.8 ч|
|10⁶|~55.5 ч|~5.5 ч|
|10⁹|~6.3 лет|~8.3 ч|

Формулы:

- Время линейного поиска на A (в секундах) = `0.2 * n`
    
- Время бинарного поиска на B (в секундах) = `1000 * log(n)`
    

---

### Всегда ли работает асимптотический анализ?

Асимптотический анализ не идеален, но это **лучший доступный метод** для оценки алгоритмов.

Например:

- Есть два алгоритма сортировки: `1000·n log n` и `2·n log n`
    
- Оба имеют одинаковый асимптотический рост `n log n`
    
- Асимптотический анализ **не позволит** сказать, какой алгоритм лучше, потому что он **игнорирует константы**
    

В реальности:

- Асимптотически Heap Sort лучше Quick Sort
    
- Но на практике Quick Sort может работать быстрее
    

Также стоит учитывать, что асимптотический анализ ориентирован **на большие размеры входа**.  
Если в вашей программе такие большие данные никогда не встречаются, то **асимптотически медленный алгоритм** может быть быстрее для вашей конкретной ситуации.

### Анализ алгоритмов: худший, средний и лучший случаи

**Последнее обновление:** 23 июля 2025

В предыдущем материале мы обсуждали, как асимптотический анализ решает проблемы наивного подхода к оценке алгоритмов. Теперь разберём, что такое **худший, средний и лучший случаи** алгоритма.

---

#### 1. Анализ худшего случая (Worst Case Analysis, используется чаще всего)

При анализе **худшего случая** мы оцениваем **верхнюю границу** времени выполнения алгоритма. Нужно понять, какой сценарий вызывает максимальное число операций.

Например, для линейного поиска худший случай происходит, когда искомый элемент **не присутствует в массиве**. В этом случае алгоритм сравнивает элемент со всеми элементами массива по очереди.

**Почему это анализ используется чаще всего?**

- Он проще среднего случая
    
- Дает **полезную информацию** о верхней границе времени выполнения
    
- Обеспечивает гарантию, что алгоритм не превысит определённое время
    

---

#### 2. Анализ лучшего случая (Best Case Analysis, используется очень редко)

При анализе **лучшего случая** мы оцениваем **нижнюю границу** времени выполнения алгоритма. Нужно понять, какой сценарий требует минимального числа операций.

Например, для линейного поиска лучший случай — когда элемент находится **в первой позиции**. В этом случае количество операций **не зависит от размера входных данных**, т.е. время выполнения является постоянным.

**Особенность:**

- Такой анализ редко полезен, так как гарантия минимального времени выполнения почти не дает информации. Алгоритм может работать мгновенно в лучшем случае, но сильно тормозить в худшем.
    

---

#### 3. Анализ среднего случая (Average Case Analysis, используется редко)

При анализе **среднего случая** мы рассматриваем все возможные входные данные, оцениваем время для каждого из них, суммируем и делим на количество случаев.

Важно знать (или предсказывать) **распределение случаев**.  
Для линейного поиска часто предполагают **равномерное распределение**: элемент может находиться на любой позиции или отсутствовать в массиве.

**Особенности:**

- Средний случай трудно рассчитать на практике, так как нужно учитывать **каждый возможный вход**, его частоту и время выполнения
    
- Используется редко в реальных задачах


| №   | **Big O**                                                                                            | **Big Omega (Ω)**                                                                                       | **Big Theta (Θ)**                                                                         |
| --- | ---------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| 1   | Похож на ≤. Скорость роста алгоритма **меньше или равна** заданному значению.                        | Похож на ≥. Скорость роста алгоритма **больше или равна** заданному значению.                           | Похож на ==. Скорость роста алгоритма **равна** заданному значению.                       |
| 2   | Обозначает **верхнюю границу** функции. Ограничивает только максимальное время/память.               | Обозначает **нижнюю границу** функции. Ограничивает минимальное время/память.                           | Ограничивает функцию **сверху и снизу**. Показывает точное асимптотическое поведение.     |
| 3   | **Верхняя граница**                                                                                  | **Нижняя граница**                                                                                      | **Точная (жесткая) граница**                                                              |
| 4   | Для определения Big O рассматривается **сценарий, когда алгоритм тратит максимальное время/память**. | Для определения Big Omega рассматривается **сценарий, когда алгоритм тратит минимальное время/память**. | Big Theta применим только если **скорость роста алгоритма не меняется с размером входа**. |
| 5   | Математически: 0 ≤ f(n) ≤ C·g(n) для всех n ≥ n₀                                                     | Математически: 0 ≤ C·g(n) ≤ f(n) для всех n ≥ n₀                                                        | Математически: 0 ≤ C₂·g(n) ≤ f(n) ≤ C₁·g(n) для всех n ≥ n₀                               |


---

### Почему анализ худшего случая используют чаще всего?

- **Средний случай:** слишком сложный для точного анализа
    
- **Лучший случай:** малоинформативен, так как реальная производительность может сильно отличаться в худшем случае
    
- **Худший случай:** проще расчитать и даёт **полезную верхнюю границу** времени выполнения, что важно для разработки ПО
    

---

### Интересные моменты про асимптотику

- Для некоторых алгоритмов все случаи (лучший, худший, средний) **асимптотически одинаковы**.
    
    - Пример: Merge Sort выполняет `n log n` операций во всех случаях
        
- Для большинства других алгоритмов **лучший и худший случаи различаются**.
    
    - Пример: Quick Sort с выбором опорного элемента в конце массива:
        
        - худший случай: массив уже отсортирован
            
        - лучший случай: опорный элемент делит массив на две равные половины
            
    - Пример: Insertion Sort:
        
        - худший случай: массив отсортирован в обратном порядке
            
        - лучший случай: массив уже отсортирован
            

---

### Примеры с анализом сложности

**1. Линейный поиск**

- **Лучший случай:** элемент находится в первой позиции → постоянное время
    
- **Средний случай:** элемент находится где-то в середине → линейное время
    
- **Худший случай:** элемент отсутствует → линейное время
    

---

**2. Сумма элементов массива в зависимости от длины**

- Если длина массива **чётная** → результат = 0
    
- Если длина массива **нечётная** → результат = сумма элементов
    

**Асимптотический анализ:**

- **Лучший случай:** n чётное → постоянное время
    
- **Средний случай:** n чётное или нечётное с равной вероятностью → линейное время
    
- **Худший случай:** n всегда нечётное → линейное время

---
### Асимптотическая сложность

| Сложность      | Тип роста               | Пример алгоритма                                                          | Примечание                                       |
| -------------- | ----------------------- | ------------------------------------------------------------------------- | ------------------------------------------------ |
| **O(1)**       | Константный             | Доступ к элементу массива по индексу                                      | Время не зависит от размера входа                |
| **O(log n)**   | Логарифмический         | Бинарный поиск                                                            | Время растет медленно, даже при больших n        |
| **O(n)**       | Линейный                | Линейный поиск                                                            | Время растет пропорционально размеру входа       |
| **O(n log n)** | Линейно-логарифмический | Быстрая сортировка, Merge Sort                                            | Оптимально для сортировки больших массивов       |
| **O(n²)**      | Квадратичный            | Пузырьковая сортировка, вставками                                         | Подходит только для небольших массивов (n ≤ 10⁴) |
| **O(n³)**      | Кубический              | Тройной вложенный цикл, простое динамическое программирование на матрицах | Обычно для очень маленьких n (≤ 500)             |
| **O(2ⁿ)**      | Экспоненциальный        | Перебор всех подмножеств, рекурсия Фибоначчи без мемоизации               | Быстро становится непрактичным для n > 20        |
| **O(n!)**      | Факториальный           | Полный перебор перестановок                                               | Используется только для очень маленьких n (≤ 10) |

---

## Связь с теоремой Мастера

Для алгоритмов **разделяй и властвуй** (Divide and Conquer) рекуррентные соотношения часто имеют вид:

$$
T(n) = aT(n/b) + f(n)
$$

Где:

* (a) — количество подзадач
* (n/b) — размер каждой подзадачи
* (f(n)) — работа вне рекурсивных вызовов (деление и объединение)

В таких случаях для **асимптотического анализа** удобно использовать [[Теорема Мастера]], которая позволяет быстро определить рост функции (T(n)) без построения полного дерева рекурсии.

**Кратко:**

* Если (f(n)) растёт медленнее, чем рекурсия на подзадачах → (T(n) = Θ(n^{\log_b a}))
* Если (f(n)) растёт как подзадачи → (T(n) = Θ(n^{\log_b a} \log n))
* Если (f(n)) растёт быстрее → (T(n) = Θ(f(n)))
